name: LLM Enrichment (Steam Reviews)

on:
  schedule:
    - cron: "0 */1 * * *"  # Every 1 hours
  workflow_dispatch:

concurrency:
  group: llm-enrichment
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  ingest-steam-data:
    name: Ingest Steam Data
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        shell: bash
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install python-dotenv pymssql requests
          fi

      - name: Run Steam Ingestion
        shell: bash
        env:
          SQL_SERVER_PASSWORD: ${{ secrets.SQL_SERVER_PASSWORD }}
        run: |
          set -euo pipefail
          echo "Starting Steam ingestion at $(date -u)"
          python steam_ingest.py
          echo "Finished ingestion at $(date -u)"

  run-enrichment:
    needs: ingest-steam-data
    name: Enrich Reviews
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      # Optional: set these if you want to override script defaults
      LOG_LEVEL: INFO
      LIMIT: "200"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        shell: bash
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install pymssql python-dotenv openai
          fi

      - name: Run enrichment
        shell: bash
        env:
          SQL_SERVER_PASSWORD: ${{ secrets.SQL_SERVER_PASSWORD }}
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_DEPLOYMENT }}
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
        run: |
          set -euo pipefail
          echo "Starting enrichment at $(date -u)"
          python enrich_sentiment.py \
            --limit "${LIMIT}" \
            --log-level "${LOG_LEVEL}"
          echo "Finished enrichment at $(date -u)"

      - name: Clean dataviz outputs
        shell: bash
        run: |
          set -euo pipefail
          rm -f dataviz/*.parquet dataviz/*.json dataviz/*.csv || true

      - name: Export data as parquet
        shell: bash
        env:
          SQL_SERVER_PASSWORD: ${{ secrets.SQL_SERVER_PASSWORD }}
        run: |
          set -euo pipefail
          echo "Exporting databases to parquet at $(date -u)"
          python export_dataviz.py \
            --out-dir "dataviz" \
            --log-level "${LOG_LEVEL}"
          echo "Export complete at $(date -u)"

      - name: Commit and push dataviz outputs
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add dataviz/*.parquet dataviz/*.json dataviz/*.csv || true
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update dataviz exports [skip ci]"
            git push
          fi

      - name: Upload dataviz artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dataviz-parquet
          path: |
            dataviz/*.parquet
            dataviz/*.json
            dataviz/*.csv
          if-no-files-found: warn
